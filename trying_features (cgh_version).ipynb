{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "spacy.syntax.nn_parser.Parser size changed, may indicate binary incompatibility. Expected 72 from C header, got 64 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1c2760aa4c2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\cs3244\\lib\\processor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#>>>python3 -m spacy download en\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mspacy_nlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#sklearn imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;34m\"\"\"Load a model from an installed package.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\en_core_web_sm\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, **overrides)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lang'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pipeline'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mLANGUAGES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.lang.%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'spacy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE048\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\lang\\en\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer_exceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBASE_EXCEPTIONS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm_exceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBASE_NORMS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLANG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNORM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mupdate_exc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_lookups\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimilarityHook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTextCategorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentenceSegmenter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge_noun_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_entities\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextCategorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentencizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mentityruler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEntityRuler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36minit spacy.pipeline.pipes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: spacy.syntax.nn_parser.Parser size changed, may indicate binary incompatibility. Expected 72 from C header, got 64 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import log_loss, recall_score, precision_score\n",
    "\n",
    "from lib.processor import *\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In an interview with congressional investigato...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getty - John Shearer / Staff \\r\\nComedian Patt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By Jameson Parker Election 2016 , Politics Nov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last updated at 16:53 GMT A helmet for cyclis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13 Herbal Teas With Highest Antioxidants http:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  In an interview with congressional investigato...      0\n",
       "1  Getty - John Shearer / Staff \\r\\nComedian Patt...      1\n",
       "2  By Jameson Parker Election 2016 , Politics Nov...      1\n",
       "3   Last updated at 16:53 GMT A helmet for cyclis...      0\n",
       "4  13 Herbal Teas With Highest Antioxidants http:...      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.text[151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a6118d3ab0fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcleaned_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-a6118d3ab0fc>\u001b[0m in \u001b[0;36mpre_process\u001b[1;34m(data_text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_special\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_stops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
     ]
    }
   ],
   "source": [
    "data_text = data.text[151]\n",
    "\n",
    "def pre_process(data_text):\n",
    "    ex = preprocess_text(data_text, remove_special = False, stem=False, lemmatize = False, remove_stops = False)\n",
    "    ex = ex.replace(\"\\n\", \"\")\n",
    "    ex = ex.replace(\"\\r\", \"\")\n",
    "    doc = nlp(ex)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "cleaned_text = pre_process(data_text)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text'] = data['text'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Post-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NN', 'JJ', 'JJ', ..., 'RB', '.', 'NNS'], dtype='<U32')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Post-tag\n",
    "\n",
    "def get_postag(doc):\n",
    "    postag_arr = np.zeros(0)\n",
    "    for i in doc:\n",
    "        postag_arr = np.append(postag_arr, i.tag_)\n",
    "    \n",
    "    return postag_arr\n",
    "        \n",
    "postag_arr = get_postag(doc)\n",
    "postag_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "postag_count = Counter(([postag for postag in postag_arr]))\n",
    "# postag_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNP_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNP_percent(postag_count):\n",
    "    return postag_count.get(\"NNP\") / sum(postag_count.values())\n",
    "\n",
    "\n",
    "#print(postag_count.get(\"NNP\"))\n",
    "#print(sum(postag_count.values()))\n",
    "#print(NNP_percent(postag_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNPS_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNPS_percent(postag_count):\n",
    "    return postag_count.get(\"NNPS\") / sum(postag_count.values())\n",
    "\n",
    "\n",
    "#print(postag_count.get(\"NNPS\"))\n",
    "#print(sum(postag_count.values()))\n",
    "#print(NNPS_percent(postag_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General version (postag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postag_percent(postag_count, postag_string):\n",
    "    return postag_count.get(postag_string) / sum(postag_count.values())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.136521136521137\n"
     ]
    }
   ],
   "source": [
    "# token.is_stop != True\n",
    "words =  [token.text for token in doc if token.is_punct != True]\n",
    "# print(words)\n",
    "\n",
    "def average_word_length(words):\n",
    "    count = 0\n",
    "    \n",
    "    for word in words:     \n",
    "        count += len(word)\n",
    "        \n",
    "    return count / len(words)\n",
    "\n",
    "# print(words)\n",
    "print(average_word_length(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorted Word Distribution / Word Count (sorted by most common) (useful when combined with actual pre processed text by WJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 63),\n",
       " ('of', 48),\n",
       " ('his', 30),\n",
       " ('to', 28),\n",
       " ('and', 25),\n",
       " ('Trump', 23),\n",
       " ('in', 23),\n",
       " ('Obama', 23),\n",
       " ('that', 21),\n",
       " ('a', 21),\n",
       " ('he', 16),\n",
       " ('law', 15),\n",
       " ('by', 14),\n",
       " ('was', 12),\n",
       " ('Obamas', 12),\n",
       " ('on', 11),\n",
       " ('rule', 11),\n",
       " ('as', 10),\n",
       " ('super', 10),\n",
       " ('an', 8),\n",
       " ('intel', 8),\n",
       " ('media', 8),\n",
       " ('election', 7),\n",
       " ('Trumps', 7),\n",
       " ('Hillary', 7),\n",
       " ('The', 7),\n",
       " ('for', 6),\n",
       " ('His', 6),\n",
       " ('him', 6),\n",
       " ('But', 6),\n",
       " ('America', 6),\n",
       " ('story', 6),\n",
       " ('it', 6),\n",
       " ('not', 6),\n",
       " ('at', 6),\n",
       " ('them', 6),\n",
       " ('father', 6),\n",
       " ('2016', 5),\n",
       " ('more', 5),\n",
       " ('from', 5),\n",
       " ('far', 5),\n",
       " ('issue', 5),\n",
       " ('have', 5),\n",
       " ('could', 5),\n",
       " ('presidential', 4),\n",
       " ('been', 4),\n",
       " ('you', 4),\n",
       " ('suspense', 4),\n",
       " ('mind', 4),\n",
       " ('which', 4),\n",
       " ('is', 4),\n",
       " ('with', 4),\n",
       " ('Clinton', 4),\n",
       " ('rules', 4),\n",
       " ('sexual', 4),\n",
       " ('while', 4),\n",
       " ('her', 4),\n",
       " ('As', 4),\n",
       " ('our', 4),\n",
       " ('should', 4),\n",
       " ('illegal', 4),\n",
       " ('unconsciously', 4),\n",
       " ('He', 4),\n",
       " ('birth', 4),\n",
       " ('became', 4),\n",
       " ('Stockholm', 4),\n",
       " ('Syndrome', 4),\n",
       " ('radical', 4),\n",
       " ('Islam', 4),\n",
       " ('has', 3),\n",
       " ('response', 3),\n",
       " ('results', 3),\n",
       " ('keep', 3),\n",
       " ('unconscious', 3),\n",
       " ('this', 3),\n",
       " ('through', 3),\n",
       " ('key', 3),\n",
       " ('s', 3),\n",
       " ('two', 3),\n",
       " ('Simon', 3),\n",
       " ('Americans', 3),\n",
       " ('investigation', 3),\n",
       " ('many', 3),\n",
       " ('On', 3),\n",
       " ('nt', 3),\n",
       " ('also', 3),\n",
       " ('U.S.', 3),\n",
       " ('citizenship', 3),\n",
       " ('When', 3),\n",
       " ('abused', 3),\n",
       " ('president', 3),\n",
       " ('had', 3),\n",
       " ('Islamic', 3),\n",
       " ('terrorists', 3),\n",
       " ('terrorist', 3),\n",
       " ('June', 3),\n",
       " ('would', 3),\n",
       " ('about', 3),\n",
       " ('candidate', 2),\n",
       " ('Donald', 2),\n",
       " ('d', 2),\n",
       " ('accept', 2),\n",
       " ('Ill', 2),\n",
       " ('does', 2),\n",
       " ('brilliant', 2),\n",
       " ('instincts', 2),\n",
       " ('newly', 2),\n",
       " ('discovered', 2),\n",
       " ('intelligence', 2),\n",
       " ('we', 2),\n",
       " ('all', 2),\n",
       " ('drama', 2),\n",
       " ('telling', 2),\n",
       " ('secret', 2),\n",
       " ('quick', 2),\n",
       " ('reads', 2),\n",
       " ('before', 2),\n",
       " ('what', 2),\n",
       " ('between', 2),\n",
       " ('lines', 2),\n",
       " ('other', 2),\n",
       " ('do', 2),\n",
       " ('language', 2),\n",
       " ('protests', 2),\n",
       " ('too', 2),\n",
       " ('denial', 2),\n",
       " ('your', 2),\n",
       " ('projection', 2),\n",
       " ('imagery', 2),\n",
       " ('medias', 2),\n",
       " ('going', 2),\n",
       " ('behind', 2),\n",
       " ('accepted', 2),\n",
       " ('Hillarys', 2),\n",
       " ('enabling', 2),\n",
       " ('harassment', 2),\n",
       " ('own', 2),\n",
       " ('wanted', 2),\n",
       " ('matter', 2),\n",
       " ('regarding', 2),\n",
       " ('still', 2),\n",
       " ('declared', 2),\n",
       " ('FBI', 2),\n",
       " ('Clintons', 2),\n",
       " ('email', 2),\n",
       " ('Comey', 2),\n",
       " ('under', 2),\n",
       " ('pressure', 2),\n",
       " ('Democrats', 2),\n",
       " ('who', 2),\n",
       " ('breaking', 2),\n",
       " ('deeper', 2),\n",
       " ('hints', 2),\n",
       " ('one', 2),\n",
       " ('take', 2),\n",
       " ('significant', 2),\n",
       " ('question', 2),\n",
       " ('any', 2),\n",
       " ('2008', 2),\n",
       " ('leadership', 2),\n",
       " ('Constitution', 2),\n",
       " ('certificate', 2),\n",
       " ('document', 2),\n",
       " ('examiners', 2),\n",
       " ('buried', 2),\n",
       " ('including', 2),\n",
       " ('conspiracy', 2),\n",
       " ('Do', 2),\n",
       " ('Only', 2),\n",
       " ('avoid', 2),\n",
       " ('continually', 2),\n",
       " ('To', 2),\n",
       " ('Americas', 2),\n",
       " ('laws', 2),\n",
       " ('immigration', 2),\n",
       " ('confronted', 2),\n",
       " ('Iran', 2),\n",
       " ('money', 2),\n",
       " ('but', 2),\n",
       " ('most', 2),\n",
       " ('speeches', 2),\n",
       " ('14', 2),\n",
       " ('controlled', 2),\n",
       " ('programmed', 2),\n",
       " ('early', 2),\n",
       " ('learned', 2),\n",
       " ('Muslim', 2),\n",
       " ('near', 2),\n",
       " ('abortion', 2),\n",
       " ('mother', 2),\n",
       " ('then', 2),\n",
       " ('death', 2),\n",
       " ('deep', 2),\n",
       " ('child', 2),\n",
       " ('refusal', 2),\n",
       " ('can', 2),\n",
       " ('its', 2),\n",
       " ('In', 2),\n",
       " ('confessed', 2),\n",
       " ('I', 2),\n",
       " ('robbed', 2),\n",
       " ('massive', 2),\n",
       " ('safety', 2),\n",
       " ('You', 2),\n",
       " ('fully', 2),\n",
       " ('need', 2),\n",
       " ('Freedom', 2),\n",
       " ('Outpost', 2),\n",
       " ('Email', 1),\n",
       " ('Republican', 1),\n",
       " ('criticized', 1),\n",
       " ('whether', 1),\n",
       " ('exact', 1),\n",
       " ('words', 1),\n",
       " ('Several', 1),\n",
       " ('pundits', 1),\n",
       " ('think', 1),\n",
       " ('statement', 1),\n",
       " ('cost', 1),\n",
       " ('read', 1),\n",
       " ('broadly', 1),\n",
       " ('way', 1),\n",
       " ('subliminal', 1),\n",
       " ('revealing', 1),\n",
       " ('astute', 1),\n",
       " ('emanating', 1),\n",
       " ('possess', 1),\n",
       " ('word', 1),\n",
       " ('implies', 1),\n",
       " ('unfolding', 1),\n",
       " ('Indeed', 1),\n",
       " ('depth', 1),\n",
       " ('immeasurable', 1),\n",
       " ('told', 1),\n",
       " ('situations', 1),\n",
       " ('perceives', 1),\n",
       " ('naturally', 1),\n",
       " ('attuned', 1),\n",
       " ('persons', 1),\n",
       " ('subconscious', 1),\n",
       " ('confessions', 1),\n",
       " ('They', 1),\n",
       " ('so', 1),\n",
       " ('symbolic', 1),\n",
       " ('much', 1),\n",
       " ('log', 1),\n",
       " ('eye', 1),\n",
       " ('along', 1),\n",
       " ('conveys', 1),\n",
       " ('indirectly', 1),\n",
       " ('code', 1),\n",
       " ('There', 1),\n",
       " ('insistence', 1),\n",
       " ('violating', 1),\n",
       " ('accepting', 1),\n",
       " ('asked', 1),\n",
       " ('Are', 1),\n",
       " ('play', 1),\n",
       " ('prior', 1),\n",
       " ('candidates', 1),\n",
       " ('big', 1),\n",
       " ('picture', 1),\n",
       " ('focuses', 1),\n",
       " ('presidents', 1),\n",
       " ('playing', 1),\n",
       " ('specifically', 1),\n",
       " ('Unconsciously', 1),\n",
       " ('answer', 1),\n",
       " ('implied', 1),\n",
       " ('candidacy', 1),\n",
       " ('Democratic', 1),\n",
       " ('opponent', 1),\n",
       " ('raises', 1),\n",
       " ('serious', 1),\n",
       " ('questions', 1),\n",
       " ('For', 1),\n",
       " ('weeks', 1),\n",
       " ('harangued', 1),\n",
       " ('complaining', 1),\n",
       " ('played', 1),\n",
       " ('societys', 1),\n",
       " ('conduct', 1),\n",
       " ('That', 1),\n",
       " ('criticism', 1),\n",
       " ('continued', 1),\n",
       " ('they', 1),\n",
       " ('ignored', 1),\n",
       " ('longtime', 1),\n",
       " ('husbands', 1),\n",
       " ('abusive', 1),\n",
       " ('behavior', 1),\n",
       " ('documented', 1),\n",
       " ('victims', 1),\n",
       " ('well', 1),\n",
       " ('escapades', 1),\n",
       " ('turn', 1),\n",
       " ('into', 1),\n",
       " ('central', 1),\n",
       " ('important', 1),\n",
       " ('nations', 1),\n",
       " ('foundation', 1),\n",
       " ('sits', 1),\n",
       " ('table', 1),\n",
       " ('Roger', 1),\n",
       " ('L.', 1),\n",
       " ('pjmedia.com', 1),\n",
       " ('wrote', 1),\n",
       " ('March', 1),\n",
       " ('23', 1),\n",
       " ('very', 1),\n",
       " ('backbone', 1),\n",
       " ('country', 1),\n",
       " ('Without', 1),\n",
       " ('know', 1),\n",
       " ('exist', 1),\n",
       " ('must', 1),\n",
       " ('be', 1),\n",
       " ('convinced', 1),\n",
       " ('handled', 1),\n",
       " ('justly', 1),\n",
       " ('dare', 1),\n",
       " ('interfere', 1),\n",
       " ('Neither', 1),\n",
       " ('condition', 1),\n",
       " ('satisfied', 1),\n",
       " ('avowed', 1),\n",
       " ('long', 1),\n",
       " ('Department', 1),\n",
       " ('Justice', 1),\n",
       " ('ruled', 1),\n",
       " ('innocent', 1),\n",
       " ('Then', 1),\n",
       " ('Director', 1),\n",
       " ('James', 1),\n",
       " ('July', 1),\n",
       " ('detailed', 1),\n",
       " ('violations', 1),\n",
       " ('confessionreading', 1),\n",
       " ('muchrevealed', 1),\n",
       " ('she', 1),\n",
       " ('prosecuted', 1),\n",
       " ('although', 1),\n",
       " ('lacked', 1),\n",
       " ('courage', 1),\n",
       " ('see', 1),\n",
       " ('October', 1),\n",
       " ('28', 1),\n",
       " ('th', 1),\n",
       " ('emails', 1),\n",
       " ('reopened', 1),\n",
       " ('warned', 1),\n",
       " ('nominated', 1),\n",
       " ('tried', 1),\n",
       " ('reiterated', 1),\n",
       " ('allowed', 1),\n",
       " ('run', 1),\n",
       " ('reading', 1),\n",
       " ('Yet', 1),\n",
       " ('there', 1),\n",
       " ('powerful', 1),\n",
       " ('terrifies', 1),\n",
       " ('entire', 1),\n",
       " ('political', 1),\n",
       " ('class', 1),\n",
       " ('demonstrates', 1),\n",
       " ('subtle', 1),\n",
       " ('ahead', 1),\n",
       " ('conscious', 1),\n",
       " ('Deep', 1),\n",
       " ('down', 1),\n",
       " ('focused', 1),\n",
       " ('Barack', 1),\n",
       " ('facilitated', 1),\n",
       " ('violation', 1),\n",
       " ('scandal', 1),\n",
       " ('alludes', 1),\n",
       " ('continues', 1),\n",
       " ('asking', 1),\n",
       " ('greatest', 1),\n",
       " ('legality', 1),\n",
       " ('history', 1),\n",
       " ('2012', 1),\n",
       " ('From', 1),\n",
       " ('2015', 1),\n",
       " ('stood', 1),\n",
       " ('strong', 1),\n",
       " ('demonstrating', 1),\n",
       " ('type', 1),\n",
       " ('insisting', 1),\n",
       " ('respect', 1),\n",
       " ('questioned', 1),\n",
       " ('failure', 1),\n",
       " ('produce', 1),\n",
       " ('evidence', 1),\n",
       " ('legal', 1),\n",
       " ('produced', 1),\n",
       " ('alleged', 1),\n",
       " ('2011', 1),\n",
       " ('aware', 1),\n",
       " ('Sheriff', 1),\n",
       " ('Joe', 1),\n",
       " ('Arpaios', 1),\n",
       " ('panel', 1),\n",
       " ('phony', 1),\n",
       " ('Reporters', 1),\n",
       " ('millions', 1),\n",
       " ('labeling', 1),\n",
       " ('racist', 1),\n",
       " ('theoristsa', 1),\n",
       " ('basic', 1),\n",
       " ('reflecting', 1),\n",
       " ('PC', 1),\n",
       " ('itself', 1),\n",
       " ('expense', 1),\n",
       " ('believe', 1),\n",
       " ('no', 1),\n",
       " ('longer', 1),\n",
       " ('birther', 1),\n",
       " ('recently', 1),\n",
       " ('did', 1),\n",
       " ('claim', 1),\n",
       " ('besides', 1),\n",
       " ('saw', 1),\n",
       " ('functioned', 1),\n",
       " ('detriment', 1),\n",
       " ('repeatedly', 1),\n",
       " ('violated', 1),\n",
       " ('whim', 1),\n",
       " ('example', 1),\n",
       " ('ignoring', 1),\n",
       " ('neglected', 1),\n",
       " ('protect', 1),\n",
       " ('national', 1),\n",
       " ('security', 1),\n",
       " ('recognized', 1),\n",
       " ('increasingly', 1),\n",
       " ('enabled', 1),\n",
       " ('simultaneously', 1),\n",
       " ('building', 1),\n",
       " ('up', 1),\n",
       " ('even', 1),\n",
       " ('giving', 1),\n",
       " ('nuclear', 1),\n",
       " ('bomb', 1),\n",
       " ('layaway', 1),\n",
       " ('plan', 1),\n",
       " ('cash', 1),\n",
       " ('boot', 1),\n",
       " ('Following', 1),\n",
       " ('11', 1),\n",
       " ('Orlando', 1),\n",
       " ('massacre', 1),\n",
       " ('perpetrated', 1),\n",
       " ('observing', 1),\n",
       " ('identify', 1),\n",
       " ('enemycall', 1),\n",
       " ('their', 1),\n",
       " ('name', 1),\n",
       " ('terroristsand', 1),\n",
       " ('suggested', 1),\n",
       " ('knew', 1),\n",
       " ('reasons', 1),\n",
       " ('resign', 1),\n",
       " ('presidency', 1),\n",
       " ('surface', 1),\n",
       " ('enraged', 1),\n",
       " ('elicited', 1),\n",
       " ('shocking', 1),\n",
       " ('confession', 1),\n",
       " ('imaginable', 1),\n",
       " ('Secretly', 1),\n",
       " ('wants', 1),\n",
       " ('truth', 1),\n",
       " ('known', 1),\n",
       " ('heels', 1),\n",
       " ('challenge', 1),\n",
       " ('explained', 1),\n",
       " ('12', 1),\n",
       " ('truly', 1),\n",
       " ('prisoner', 1),\n",
       " ('age', 1),\n",
       " ('home', 1),\n",
       " ('decoded', 1),\n",
       " ('details', 1),\n",
       " ('are', 1),\n",
       " ('described', 1),\n",
       " ('my', 1),\n",
       " ('book', 1),\n",
       " ('President', 1),\n",
       " ('How', 1),\n",
       " ('Triggered', 1),\n",
       " ('Hidden', 1),\n",
       " ('Confession', 1),\n",
       " ('intuitively', 1),\n",
       " ('put', 1),\n",
       " ('together', 1),\n",
       " ('life', 1),\n",
       " ('horrifying', 1),\n",
       " ('family', 1),\n",
       " ('aborted', 1),\n",
       " ('My', 1),\n",
       " ('scientific', 1),\n",
       " ('forensic', 1),\n",
       " ('profiling', 1),\n",
       " ('approach', 1),\n",
       " ('confirmed', 1),\n",
       " ('pointed', 1),\n",
       " ('toward', 1),\n",
       " ('matched', 1),\n",
       " ('lifes', 1),\n",
       " ('circumstances', 1),\n",
       " ('promiscuous', 1),\n",
       " ('17', 1),\n",
       " ('year', 1),\n",
       " ('old', 1),\n",
       " ('unexpectedly', 1),\n",
       " ('pregnant', 1),\n",
       " ('casual', 1),\n",
       " ('affair', 1),\n",
       " ('emotionally', 1),\n",
       " ('killed', 1),\n",
       " ('abandoning', 1),\n",
       " ('felt', 1),\n",
       " ('pain', 1),\n",
       " ('daily', 1),\n",
       " ('basis', 1),\n",
       " ('physically', 1),\n",
       " ('later', 1),\n",
       " ('wives', 1),\n",
       " ('Kenya', 1),\n",
       " ('constant', 1),\n",
       " ('experience', 1),\n",
       " ('hands', 1),\n",
       " ('anti', 1),\n",
       " ('American', 1),\n",
       " ('fixated', 1),\n",
       " ('frozen', 1),\n",
       " ('terrorsuffering', 1),\n",
       " ('childhood', 1),\n",
       " ('traumaterrified', 1),\n",
       " ('kill', 1),\n",
       " ('moment', 1),\n",
       " ('himboth', 1),\n",
       " ('same', 1),\n",
       " ('stay', 1),\n",
       " ('alive', 1),\n",
       " ('victim', 1),\n",
       " ('Islamexplaining', 1),\n",
       " ('repeated', 1),\n",
       " ('capitulations', 1),\n",
       " ('curious', 1),\n",
       " ('use', 1),\n",
       " ('term', 1),\n",
       " ('remains', 1),\n",
       " ('psyche', 1),\n",
       " ('tell', 1),\n",
       " ('psycholinguistic', 1),\n",
       " ('recent', 1),\n",
       " ('unknowingly', 1),\n",
       " ('images', 1),\n",
       " ('such', 1),\n",
       " ('doing', 1),\n",
       " ('work', 1),\n",
       " ('or', 1),\n",
       " ('denials', 1),\n",
       " ('win', 1),\n",
       " ('m', 1),\n",
       " ('let', 1),\n",
       " ('happen', 1),\n",
       " ('further', 1),\n",
       " ('speech', 1),\n",
       " ('mole', 1),\n",
       " ('carry', 1),\n",
       " ('out', 1),\n",
       " ('major', 1),\n",
       " ('robbery', 1),\n",
       " ('first', 1),\n",
       " ('stealing', 1),\n",
       " ('running', 1),\n",
       " ('fact', 1),\n",
       " ('ways', 1),\n",
       " ('economically', 1),\n",
       " ('debt', 1),\n",
       " ('hampering', 1),\n",
       " ('business', 1),\n",
       " ('us', 1),\n",
       " ('borders', 1),\n",
       " ('gains', 1),\n",
       " ('racial', 1),\n",
       " ('harmony', 1),\n",
       " ('safe', 1),\n",
       " ('communities', 1),\n",
       " ('police', 1),\n",
       " ('inner', 1),\n",
       " ('city', 1),\n",
       " ('affordable', 1),\n",
       " ('health', 1),\n",
       " ('care', 1),\n",
       " ('military', 1),\n",
       " ('power', 1),\n",
       " ('world', 1),\n",
       " ('role', 1),\n",
       " ('All', 1),\n",
       " ('policies', 1),\n",
       " ('strengthened', 1),\n",
       " ('Now', 1),\n",
       " ('return', 1),\n",
       " ('threatened', 1),\n",
       " ('Consider', 1),\n",
       " ('message', 1),\n",
       " ('how', 1),\n",
       " ('both', 1),\n",
       " ('especially', 1),\n",
       " ('will', 1),\n",
       " ('Consciously', 1),\n",
       " ('may', 1),\n",
       " ('yet', 1),\n",
       " ('appreciate', 1),\n",
       " ('plans', 1),\n",
       " ('strongly', 1),\n",
       " ('If', 1),\n",
       " ('wins', 1),\n",
       " ('appoint', 1),\n",
       " ('special', 1),\n",
       " ('committee', 1),\n",
       " ('study', 1),\n",
       " ('reversed', 1),\n",
       " ('themselves', 1),\n",
       " ('gay', 1),\n",
       " ('marriage', 1),\n",
       " ('simply', 1),\n",
       " ('change', 1),\n",
       " ('clarify', 1),\n",
       " ('birthplace', 1),\n",
       " ('if', 1),\n",
       " ('loses', 1),\n",
       " ('rise', 1),\n",
       " ('occasion', 1),\n",
       " ('putting', 1),\n",
       " ('renewed', 1),\n",
       " ('potentially', 1),\n",
       " ('mobilize', 1),\n",
       " ('citizenry', 1),\n",
       " ('Rule', 1),\n",
       " ('demand', 1),\n",
       " ('Congress', 1),\n",
       " ('action', 1),\n",
       " ('legally', 1),\n",
       " ('obtain', 1),\n",
       " ('recordsas', 1),\n",
       " ('himself', 1),\n",
       " ('traumatized', 1),\n",
       " ('encouraged', 1),\n",
       " ('like', 1),\n",
       " ('others', 1),\n",
       " ('espouses', 1),\n",
       " ('And', 1),\n",
       " ('re', 1),\n",
       " ('embrace', 1),\n",
       " ('moral', 1),\n",
       " ('compass', 1),\n",
       " (\"n't\", 1),\n",
       " ('forget', 1),\n",
       " ('Like', 1),\n",
       " ('Facebook', 1),\n",
       " ('Google', 1),\n",
       " ('Plus', 1),\n",
       " ('Twitter', 1),\n",
       " ('get', 1),\n",
       " ('delivered', 1),\n",
       " ('Amazon', 1),\n",
       " ('Kindle', 1),\n",
       " ('device', 1),\n",
       " ('here', 1),\n",
       " ('shares', 1)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter = Counter(words)\n",
    "word_counter.most_common()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.137777777777778\n"
     ]
    }
   ],
   "source": [
    "def average_word_count(word_counter):\n",
    "    # print(word_counter.values())\n",
    "    return np.array(list(word_counter.values())).mean()\n",
    "\n",
    "# print(words)\n",
    "print(average_word_count(word_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443\n"
     ]
    }
   ],
   "source": [
    "def word_count(word_counter):\n",
    "    return sum(word_counter.values())\n",
    "\n",
    "# print(word_count(words))\n",
    "print(sum(word_counter.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686\n"
     ]
    }
   ],
   "source": [
    "def unique_words(word_counter):\n",
    "    return len(word_counter)\n",
    "\n",
    "print(unique_words(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(word_counter):\n",
    "    return unique_words(word_counter) / word_count(word_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verb count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postag(doc):\n",
    "    pos_arr = np.zeros(0)\n",
    "    for i in doc:\n",
    "        pos_arr = np.append(postag_arr, i.pos_)\n",
    "    \n",
    "    return pos_arr\n",
    "    \n",
    "\n",
    "def verb_count(pos_arr):\n",
    "    pos_count = Counter(([pos for pos in pos_arr]))\n",
    "    verb_count = pos_count.get(\"VERB\") / sum(pos_count.values())\n",
    "    # print(pos_count)\n",
    "    # print(pos_count.get(\"VERB\"))\n",
    "    # print(sum(pos_count.values()))\n",
    "    # print(verb_count)\n",
    "    \n",
    "    return verb_count\n",
    "\n",
    "#(ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NOUN', 'NOUN', 'PROPN', ..., 'ADJ', 'NOUN', 'PUNCT'], dtype='<U32')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General version (pos) (less powerful version of postag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_count(pos_arr, pos_string):\n",
    "    pos_count = Counter(([pos for pos in pos_arr]))\n",
    "    result = pos_count.get(pos_string) / sum(pos_count.values())\n",
    "    # print(pos_count)\n",
    "    # print(pos_count.get(\"VERB\"))\n",
    "    # print(sum(pos_count.values()))\n",
    "    # print(verb_count)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Might be of importance!!! (Similarity of 2 texts function) https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CGH\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\CGH\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8520246756820952"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp1 = nlp(u\"An apple a day keeps the doctor away.\")\n",
    "nlp2 = nlp(u\"How many doctors eat apples everyday?\")\n",
    "\n",
    "nlp1.similarity(nlp2)\n",
    "\n",
    "# How to deal with model warning?\n",
    "\n",
    "nlp3 = nlp(u\"123\")\n",
    "nlp4 = nlp(u\"1234\")\n",
    "\n",
    "nlp3.similarity(nlp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)\n",
    "\n",
    "def sentence_count(doc):\n",
    "    return len(list(doc.sents))\n",
    "\n",
    "sentence_count(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6ab5fcfe59bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is skipped for now: sensory ratio, spatial and temporal ratio, and imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data.text, data.label\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=0, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27208,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30232,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag_arr = np.zeros(0)\n",
    "for i in doc:\n",
    "    postag_arr = np.append(postag_arr, i.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NN', 'NN', 'NNP', ..., 'JJ', 'NN', '.'], dtype='<U32')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postag_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_percent(x):\n",
    "    return sum([k == \"NN\" for k in x])/ len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image',\n",
       " 'copyright',\n",
       " 'Getty',\n",
       " 'Images',\n",
       " 'On',\n",
       " 'Sunday',\n",
       " 'morning',\n",
       " 'Donald',\n",
       " 'Trump',\n",
       " 'went',\n",
       " 'off',\n",
       " 'on',\n",
       " 'a',\n",
       " 'Twitter',\n",
       " 'tirade',\n",
       " 'against',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'his',\n",
       " 'own',\n",
       " 'party',\n",
       " 'This',\n",
       " 'in',\n",
       " 'itself',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'exactly',\n",
       " 'huge',\n",
       " 'news',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'far',\n",
       " 'from',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'the',\n",
       " 'president',\n",
       " 'has',\n",
       " 'turned',\n",
       " 'his',\n",
       " 'rhetorical',\n",
       " 'cannons',\n",
       " 'on',\n",
       " 'his',\n",
       " 'own',\n",
       " 'ranks',\n",
       " 'This',\n",
       " 'time',\n",
       " 'however',\n",
       " 'his',\n",
       " 'attacks',\n",
       " 'were',\n",
       " 'particularly',\n",
       " 'biting',\n",
       " 'and',\n",
       " 'personal',\n",
       " 'He',\n",
       " 'essentially',\n",
       " 'called',\n",
       " 'Tennessee',\n",
       " 'Senator',\n",
       " 'Bob',\n",
       " 'Corker',\n",
       " 'the',\n",
       " 'chair',\n",
       " 'of',\n",
       " 'the',\n",
       " 'powerful',\n",
       " 'Senate',\n",
       " 'Foreign',\n",
       " 'Relations',\n",
       " 'Committee',\n",
       " 'a',\n",
       " 'coward',\n",
       " 'for',\n",
       " 'not',\n",
       " 'running',\n",
       " 'for',\n",
       " 're',\n",
       " 'election',\n",
       " 'He',\n",
       " 'said',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " 'begged',\n",
       " 'for',\n",
       " 'the',\n",
       " 'president',\n",
       " \"'s\",\n",
       " 'endorsement',\n",
       " 'which',\n",
       " 'he',\n",
       " 'refused',\n",
       " 'to',\n",
       " 'give',\n",
       " 'He',\n",
       " 'wrongly',\n",
       " 'claimed',\n",
       " 'that',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " \"'s\",\n",
       " 'support',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Iranian',\n",
       " 'nuclear',\n",
       " 'agreement',\n",
       " 'was',\n",
       " 'his',\n",
       " 'only',\n",
       " 'political',\n",
       " 'accomplishment',\n",
       " 'Unlike',\n",
       " 'some',\n",
       " 'of',\n",
       " 'his',\n",
       " 'colleagues',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " 'free',\n",
       " 'from',\n",
       " 'having',\n",
       " 'to',\n",
       " 'worry',\n",
       " 'about',\n",
       " 'his',\n",
       " 'immediate',\n",
       " 'political',\n",
       " 'future',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'hold',\n",
       " 'his',\n",
       " 'tongue',\n",
       " 'Skip',\n",
       " 'Twitter',\n",
       " 'post',\n",
       " 'by',\n",
       " '@SenBobCorker',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'shame',\n",
       " 'the',\n",
       " 'White',\n",
       " 'House',\n",
       " 'has',\n",
       " 'become',\n",
       " 'an',\n",
       " 'adult',\n",
       " 'day',\n",
       " 'care',\n",
       " 'center',\n",
       " 'Someone',\n",
       " 'obviously',\n",
       " 'missed',\n",
       " 'their',\n",
       " 'shift',\n",
       " 'this',\n",
       " 'morning',\n",
       " 'Senator',\n",
       " 'Bob',\n",
       " 'Corker',\n",
       " '@SenBobCorker',\n",
       " 'October',\n",
       " '8',\n",
       " '2017',\n",
       " 'Report',\n",
       " 'That',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'it',\n",
       " 'though',\n",
       " 'He',\n",
       " 'then',\n",
       " 'spoke',\n",
       " 'with',\n",
       " 'the',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times',\n",
       " 'and',\n",
       " 'really',\n",
       " 'let',\n",
       " 'the',\n",
       " 'president',\n",
       " 'have',\n",
       " 'it',\n",
       " 'Here',\n",
       " 'are',\n",
       " 'four',\n",
       " 'choice',\n",
       " 'quotes',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Tennessee',\n",
       " 'senator',\n",
       " \"'s\",\n",
       " 'interview',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Times',\n",
       " 'and',\n",
       " 'why',\n",
       " 'they',\n",
       " 'are',\n",
       " 'particularly',\n",
       " 'damning',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'why',\n",
       " 'the',\n",
       " 'president',\n",
       " 'tweets',\n",
       " 'out',\n",
       " 'things',\n",
       " 'that',\n",
       " 'are',\n",
       " 'not',\n",
       " 'true',\n",
       " 'You',\n",
       " 'know',\n",
       " 'he',\n",
       " 'does',\n",
       " 'it',\n",
       " 'everyone',\n",
       " 'knows',\n",
       " 'he',\n",
       " 'does',\n",
       " 'it',\n",
       " 'but',\n",
       " 'he',\n",
       " 'does',\n",
       " 'You',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'sugarcoat',\n",
       " 'this',\n",
       " 'one',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " 'is',\n",
       " 'flat',\n",
       " 'out',\n",
       " 'saying',\n",
       " 'the',\n",
       " 'president',\n",
       " 'is',\n",
       " 'a',\n",
       " 'liar',\n",
       " 'and',\n",
       " 'everyone',\n",
       " 'knows',\n",
       " 'it',\n",
       " 'The',\n",
       " 'senator',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'is',\n",
       " 'challenging',\n",
       " 'Mr',\n",
       " 'Trump',\n",
       " \"'s\",\n",
       " 'insistence',\n",
       " 'that',\n",
       " 'he',\n",
       " 'unsuccessfully',\n",
       " 'pleaded',\n",
       " 'for',\n",
       " 'his',\n",
       " 'endorsement',\n",
       " 'but',\n",
       " 'the',\n",
       " 'accusation',\n",
       " 'is',\n",
       " 'much',\n",
       " 'broader',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " 'and',\n",
       " 'the',\n",
       " 'president',\n",
       " 'used',\n",
       " 'to',\n",
       " 'be',\n",
       " 'something',\n",
       " 'akin',\n",
       " 'to',\n",
       " 'allies',\n",
       " 'The',\n",
       " 'Tennessean',\n",
       " 'was',\n",
       " 'on',\n",
       " 'Mr',\n",
       " 'Trump',\n",
       " \"'s\",\n",
       " 'short',\n",
       " 'list',\n",
       " 'for',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'and',\n",
       " 'secretary',\n",
       " 'of',\n",
       " 'state',\n",
       " 'Image',\n",
       " 'copyright',\n",
       " 'Getty',\n",
       " 'Images',\n",
       " 'Image',\n",
       " 'caption',\n",
       " 'Bob',\n",
       " 'Corker',\n",
       " 'at',\n",
       " 'Trump',\n",
       " 'campaign',\n",
       " 'rally',\n",
       " 'in',\n",
       " 'July',\n",
       " '2016',\n",
       " 'Those',\n",
       " 'days',\n",
       " 'are',\n",
       " 'seemingly',\n",
       " 'very',\n",
       " 'much',\n",
       " 'over',\n",
       " 'now',\n",
       " 'and',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'not',\n",
       " 'like',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " 'is',\n",
       " 'going',\n",
       " 'anywhere',\n",
       " 'anytime',\n",
       " 'soon',\n",
       " 'Although',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'not',\n",
       " 'running',\n",
       " 'for',\n",
       " 're',\n",
       " 'election',\n",
       " 'he',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Senate',\n",
       " 'chairing',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'committee',\n",
       " 'until',\n",
       " 'January',\n",
       " '2019',\n",
       " 'The',\n",
       " 'president',\n",
       " \"'s\",\n",
       " 'margin',\n",
       " 'for',\n",
       " 'success',\n",
       " 'in',\n",
       " 'that',\n",
       " 'chamber',\n",
       " 'is',\n",
       " 'razor',\n",
       " 'thin',\n",
       " 'If',\n",
       " 'Democrats',\n",
       " 'can',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'stand',\n",
       " 'together',\n",
       " 'in',\n",
       " 'opposition',\n",
       " 'he',\n",
       " 'can',\n",
       " 'afford',\n",
       " 'to',\n",
       " 'lose',\n",
       " 'only',\n",
       " 'two',\n",
       " 'votes',\n",
       " 'out',\n",
       " 'of',\n",
       " '52',\n",
       " 'Republican',\n",
       " 'senators',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'why',\n",
       " 'healthcare',\n",
       " 'reform',\n",
       " 'collapsed',\n",
       " 'in',\n",
       " 'July',\n",
       " 'and',\n",
       " 'it',\n",
       " 'could',\n",
       " 'be',\n",
       " 'bad',\n",
       " 'news',\n",
       " 'for',\n",
       " 'tax',\n",
       " 'efforts',\n",
       " 'From',\n",
       " 'here',\n",
       " 'on',\n",
       " 'out',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'the',\n",
       " 'president',\n",
       " 'any',\n",
       " 'favours',\n",
       " 'Look',\n",
       " 'except',\n",
       " 'for',\n",
       " 'a',\n",
       " 'few',\n",
       " 'people',\n",
       " 'the',\n",
       " 'vast',\n",
       " 'majority',\n",
       " 'of',\n",
       " 'our',\n",
       " 'caucus',\n",
       " 'understands',\n",
       " 'what',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'here',\n",
       " 'Frustration',\n",
       " 'in',\n",
       " 'Congress',\n",
       " 'has',\n",
       " 'been',\n",
       " 'growing',\n",
       " 'over',\n",
       " 'what',\n",
       " 'Republicans',\n",
       " 'feel',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'president',\n",
       " \"'s\",\n",
       " 'inability',\n",
       " 'to',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'advancing',\n",
       " 'their',\n",
       " 'agenda',\n",
       " 'Getting',\n",
       " 'a',\n",
       " 'sharply',\n",
       " 'divided',\n",
       " 'party',\n",
       " 'to',\n",
       " 'come',\n",
       " 'together',\n",
       " 'on',\n",
       " 'plans',\n",
       " 'to',\n",
       " 'repeal',\n",
       " 'Obamacare',\n",
       " 'reform',\n",
       " 'taxes',\n",
       " 'or',\n",
       " 'boost',\n",
       " 'infrastructure',\n",
       " 'spending',\n",
       " 'is',\n",
       " 'challenging',\n",
       " 'enough',\n",
       " 'Doing',\n",
       " 'so',\n",
       " 'when',\n",
       " 'the',\n",
       " 'president',\n",
       " 'stirs',\n",
       " 'up',\n",
       " 'unrelated',\n",
       " 'controversies',\n",
       " 'on',\n",
       " 'a',\n",
       " 'seemingly',\n",
       " 'daily',\n",
       " 'basis',\n",
       " 'makes',\n",
       " 'things',\n",
       " 'all',\n",
       " 'the',\n",
       " 'harder',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'president',\n",
       " \"'s\",\n",
       " 'gifts',\n",
       " 'has',\n",
       " 'been',\n",
       " 'his',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'shake',\n",
       " 'off',\n",
       " 'negative',\n",
       " 'stories',\n",
       " 'by',\n",
       " 'quickly',\n",
       " 'moving',\n",
       " 'on',\n",
       " 'to',\n",
       " 'a',\n",
       " 'different',\n",
       " 'subject',\n",
       " 'That',\n",
       " 'worked',\n",
       " 'brilliantly',\n",
       " 'during',\n",
       " 'his',\n",
       " 'presidential',\n",
       " 'campaign',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'less',\n",
       " 'effective',\n",
       " 'during',\n",
       " 'the',\n",
       " 'legislative',\n",
       " 'slow',\n",
       " 'grind',\n",
       " 'Image',\n",
       " 'copyright',\n",
       " 'Getty',\n",
       " 'Images',\n",
       " 'Image',\n",
       " 'caption',\n",
       " 'Corker',\n",
       " 'at',\n",
       " 'the',\n",
       " 'confirmation',\n",
       " 'hearing',\n",
       " 'for',\n",
       " 'Secretary',\n",
       " 'of',\n",
       " 'State',\n",
       " 'Rex',\n",
       " 'Tillerson',\n",
       " 'For',\n",
       " 'months',\n",
       " 'Republicans',\n",
       " 'in',\n",
       " 'Congress',\n",
       " 'have',\n",
       " 'been',\n",
       " 'grumbling',\n",
       " 'about',\n",
       " 'this',\n",
       " 'in',\n",
       " 'the',\n",
       " 'background',\n",
       " 'and',\n",
       " 'among',\n",
       " 'themselves',\n",
       " 'Occasionally',\n",
       " 'someone',\n",
       " 'like',\n",
       " 'Mr',\n",
       " 'McConnell',\n",
       " 'will',\n",
       " 'lament',\n",
       " 'that',\n",
       " 'the',\n",
       " 'president',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'understand',\n",
       " 'how',\n",
       " 'the',\n",
       " 'Senate',\n",
       " 'works',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " 'has',\n",
       " 'now',\n",
       " 'stated',\n",
       " 'it',\n",
       " 'loud',\n",
       " 'and',\n",
       " 'clear',\n",
       " 'And',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'more',\n",
       " 'he',\n",
       " 'says',\n",
       " 'almost',\n",
       " 'everyone',\n",
       " 'agrees',\n",
       " 'with',\n",
       " 'him',\n",
       " 'They',\n",
       " \"'ve\",\n",
       " 'kept',\n",
       " 'silent',\n",
       " 'until',\n",
       " 'now',\n",
       " 'because',\n",
       " 'they',\n",
       " 'still',\n",
       " 'hope',\n",
       " 'to',\n",
       " 'pass',\n",
       " 'conservative',\n",
       " 'legislation',\n",
       " 'that',\n",
       " 'the',\n",
       " 'president',\n",
       " 'can',\n",
       " 'sign',\n",
       " 'or',\n",
       " 'fear',\n",
       " 'Mr',\n",
       " 'Trump',\n",
       " \"'s\",\n",
       " 'legions',\n",
       " 'will',\n",
       " 'back',\n",
       " 'a',\n",
       " 'primary',\n",
       " 'challenge',\n",
       " 'next',\n",
       " 'year',\n",
       " 'or',\n",
       " 'stay',\n",
       " 'home',\n",
       " 'during',\n",
       " 'the',\n",
       " 'general',\n",
       " 'election',\n",
       " 'If',\n",
       " 'that',\n",
       " 'calculus',\n",
       " 'ever',\n",
       " 'changes',\n",
       " 'if',\n",
       " 'it',\n",
       " 'becomes',\n",
       " 'riskier',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'silent',\n",
       " 'than',\n",
       " 'speak',\n",
       " 'out',\n",
       " 'Mr',\n",
       " 'Trump',\n",
       " 'will',\n",
       " 'be',\n",
       " 'in',\n",
       " 'real',\n",
       " 'trouble',\n",
       " 'A',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'people',\n",
       " 'think',\n",
       " 'that',\n",
       " 'there',\n",
       " 'is',\n",
       " 'some',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'good',\n",
       " 'cop',\n",
       " 'bad',\n",
       " 'cop',\n",
       " 'act',\n",
       " 'underway',\n",
       " 'but',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'not',\n",
       " 'true',\n",
       " 'Time',\n",
       " 'and',\n",
       " 'again',\n",
       " 'Mr',\n",
       " 'Trump',\n",
       " 'has',\n",
       " 'appeared',\n",
       " 'to',\n",
       " 'undercut',\n",
       " 'Secretary',\n",
       " 'of',\n",
       " 'State',\n",
       " 'Rex',\n",
       " 'Tillerson',\n",
       " 'and',\n",
       " 'others',\n",
       " 'in',\n",
       " 'his',\n",
       " 'administration',\n",
       " 'who',\n",
       " 'are',\n",
       " 'attempting',\n",
       " 'to',\n",
       " 'use',\n",
       " 'soft',\n",
       " 'diplomacy',\n",
       " 'to',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'a',\n",
       " 'range',\n",
       " 'of',\n",
       " 'international',\n",
       " 'crises',\n",
       " 'The',\n",
       " 'war',\n",
       " 'against',\n",
       " 'the',\n",
       " 'Taliban',\n",
       " 'in',\n",
       " 'Afghanistan',\n",
       " 'Iran',\n",
       " \"'s\",\n",
       " 'compliance',\n",
       " 'with',\n",
       " 'the',\n",
       " 'multinational',\n",
       " 'nuclear',\n",
       " 'agreement',\n",
       " 'the',\n",
       " 'ongoing',\n",
       " 'dispute',\n",
       " 'between',\n",
       " 'Qatar',\n",
       " 'and',\n",
       " 'its',\n",
       " 'Persian',\n",
       " 'Gulf',\n",
       " 'neighbours',\n",
       " 'the',\n",
       " 'unrest',\n",
       " 'in',\n",
       " 'Venezuela',\n",
       " 'and',\n",
       " 'most',\n",
       " 'recently',\n",
       " 'North',\n",
       " 'Korea',\n",
       " \"'s\",\n",
       " 'continued',\n",
       " 'ballistic',\n",
       " 'missile',\n",
       " 'tests',\n",
       " 'have',\n",
       " 'all',\n",
       " 'been',\n",
       " 'the',\n",
       " 'target',\n",
       " 'of',\n",
       " 'the',\n",
       " 'president',\n",
       " \"'s\",\n",
       " 'offhand',\n",
       " 'remarks',\n",
       " 'and',\n",
       " 'Twitter',\n",
       " 'invective',\n",
       " 'Some',\n",
       " 'administration',\n",
       " 'defenders',\n",
       " 'have',\n",
       " 'said',\n",
       " 'this',\n",
       " 'is',\n",
       " 'all',\n",
       " 'a',\n",
       " 'part',\n",
       " 'of',\n",
       " 'Mr',\n",
       " 'Trump',\n",
       " \"'s\",\n",
       " 'strategy',\n",
       " 'an',\n",
       " 'updated',\n",
       " 'version',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Nixon',\n",
       " 'era',\n",
       " 'madman',\n",
       " 'theory',\n",
       " 'in',\n",
       " 'which',\n",
       " 'the',\n",
       " 'president',\n",
       " 'forces',\n",
       " 'adversaries',\n",
       " 'to',\n",
       " 'give',\n",
       " 'way',\n",
       " 'because',\n",
       " 'they',\n",
       " 'fear',\n",
       " 'an',\n",
       " 'unpredictable',\n",
       " 'US',\n",
       " 'leader',\n",
       " \"'s\",\n",
       " 'actions',\n",
       " 'Mr',\n",
       " 'Corker',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'buying',\n",
       " 'it',\n",
       " 'There',\n",
       " \"'s\",\n",
       " 'no',\n",
       " 'strategy',\n",
       " 'he',\n",
       " 'says',\n",
       " 'just',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'of',\n",
       " 'chaos',\n",
       " 'which',\n",
       " 'he',\n",
       " 'hopes',\n",
       " 'Mr',\n",
       " 'Trump',\n",
       " \"'s\",\n",
       " 'senior',\n",
       " 'advisers',\n",
       " 'will',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'I',\n",
       " 'know',\n",
       " 'for',\n",
       " 'a',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'every',\n",
       " 'single',\n",
       " 'day',\n",
       " 'at',\n",
       " 'the',\n",
       " 'White',\n",
       " 'House',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'situation',\n",
       " 'of',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'contain',\n",
       " 'him',\n",
       " 'There',\n",
       " \"'s\",\n",
       " 'now',\n",
       " 'a',\n",
       " 'growing',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'John',\n",
       " 'Kelly',\n",
       " 'face',\n",
       " 'palm',\n",
       " 'photos',\n",
       " 'that',\n",
       " 'serve',\n",
       " 'as',\n",
       " 'a',\n",
       " 'testament',\n",
       " 'to',\n",
       " 'the',\n",
       " 'chief',\n",
       " 'of',\n",
       " 'staff',\n",
       " \"'s\",\n",
       " 'reported',\n",
       " 'frustration',\n",
       " 'at',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'the',\n",
       " 'president',\n",
       " 'Mr',\n",
       " 'Trump',\n",
       " 'goes',\n",
       " 'off',\n",
       " 'script',\n",
       " 'to',\n",
       " 'praise',\n",
       " 'torch',\n",
       " 'bearing',\n",
       " 'white',\n",
       " 'nationalists',\n",
       " 'at',\n",
       " 'a',\n",
       " 'rally',\n",
       " 'in',\n",
       " 'Charlottesville',\n",
       " 'and',\n",
       " 'Mr',\n",
       " 'Kelly',\n",
       " 'is',\n",
       " 'captured',\n",
       " 'closing',\n",
       " 'his',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'rubbing',\n",
       " 'the',\n",
       " 'arch',\n",
       " 'of',\n",
       " 'his',\n",
       " 'nose',\n",
       " 'as',\n",
       " 'if',\n",
       " 'attempting',\n",
       " 'to',\n",
       " 'stave',\n",
       " 'off',\n",
       " 'a',\n",
       " 'migraine',\n",
       " 'Image',\n",
       " 'copyright',\n",
       " 'Reuters',\n",
       " 'Image',\n",
       " 'caption',\n",
       " 'White',\n",
       " 'House',\n",
       " 'Chief',\n",
       " 'of',\n",
       " 'Staff',\n",
       " 'John',\n",
       " 'Kelly',\n",
       " 'looks',\n",
       " 'on',\n",
       " 'as',\n",
       " 'US',\n",
       " 'President',\n",
       " 'Donald',\n",
       " 'Trump',\n",
       " 'speaks',\n",
       " 'at',\n",
       " 'a',\n",
       " 'campaign',\n",
       " 'rally',\n",
       " 'The',\n",
       " 'president',\n",
       " 'calls',\n",
       " 'North',\n",
       " 'Korean',\n",
       " 'leaders',\n",
       " 'criminals',\n",
       " 'in',\n",
       " 'a',\n",
       " 'speech',\n",
       " 'to',\n",
       " 'the',\n",
       " 'United',\n",
       " 'Nations',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1d45c29f2ba2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stop\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ex' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc = nlp(ex)\n",
    "\n",
    "words =  [token.text for token in doc if token.is_stop != True and token.is_punct != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize, remove stop words and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = str()\n",
    "for token in doc:\n",
    "    temp += \" \" + token.lemma_\n",
    "\n",
    "temp.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize, remove stop words and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()  \n",
    "\n",
    "counter = 0\n",
    "stem_words = []\n",
    "for token in words: \n",
    "    stem_words.append(stemmer.stem(token))\n",
    "    counter += 1\n",
    "    \n",
    "res_stem = str()\n",
    "for i in stem_words:\n",
    "    res_stem += \" \" + i\n",
    "    \n",
    "nlp(res_stem.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get PoS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag_arr = np.zeros(0)\n",
    "for i in doc:\n",
    "    postag_arr = np.append(postag_arr, i.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set([w.label_ for w in doc.ents]) \n",
    "for label in labels: \n",
    "    entities = [e.string for e in doc.ents if label==e.label_] \n",
    "    entities = list(set(entities)) \n",
    "    print(label,entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf on the lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(x2_train))\n",
    "x2_train_tfv =  tfv.transform(x2_train) \n",
    "x2_val_tfv = tfv.transform(x2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.0, solver = 'liblinear')\n",
    "clf.fit(x2_train_tfv, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_tfv = tfv.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x2_val_tfv, y2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = clf.predict(x2_val_tfv)\n",
    "\n",
    "print(\"Precision: \" + str(precision_score(y2_val, y2_pred)))\n",
    "print(\"Recall: \" + str(recall_score(y2_val, y2_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAVEN'T TRY BELOW!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "docs_dict = Dictionary(docs)\n",
    "docs_dict.filter_extremes(no_below=20, no_above=0.2)\n",
    "docs_dict.compactify()\n",
    "\n",
    "docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]\n",
    "model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "docs_tfidf  = model_tfidf[docs_corpus]\n",
    "docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
