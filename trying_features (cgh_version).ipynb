{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "#sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import log_loss, recall_score, precision_score\n",
    "\n",
    "from lib.processor import *\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In an interview with congressional investigato...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getty - John Shearer / Staff \\nComedian Patton...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By Jameson Parker Election 2016 , Politics Nov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last updated at 16:53 GMT A helmet for cyclis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13 Herbal Teas With Highest Antioxidants http:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  In an interview with congressional investigato...      0\n",
       "1  Getty - John Shearer / Staff \\nComedian Patton...      1\n",
       "2  By Jameson Parker Election 2016 , Politics Nov...      1\n",
       "3   Last updated at 16:53 GMT A helmet for cyclis...      0\n",
       "4  13 Herbal Teas With Highest Antioxidants http:...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.text[151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Email Republican presidential candidate Donald Trump has been criticized for his response as to whether hed accept 2016 election results. His exact words: Ill keep you in suspense. Several pundits think that statement cost him the election. But read more broadly, the way the subliminal mind does, and his response was brilliant, revealing his astute instincts emanating from the newly discovered unconscious super intelligence (which we all possess). His word suspense implies an unfolding drama. Indeed, the depth of this drama is immeasurable. But Trumps telling America a secret story. His story is told through his super intelligence which quick reads situations before telling what it perceives between the lines. Trumps super-intel is naturally attuned to other persons subconscious confessions. They do so in the symbolic language of protests too much denial and log in your eye projection along with other key imagery. His super-intel quick reads Hillary Clinton and the media and conveys his response indirectly in code. Theres far more to the medias insistence that Trump was violating rules by not accepting election results. The media asked, Are you going to play by the rules of prior presidential candidates? But the big picture behind this election focuses on presidents playing by the rules, specifically the rule of law. Unconsciously, Trumps answer implied that the candidacy of his Democratic opponent, Hillary Clinton, raises serious rule of law questions. For two weeks the media harangued Trump complaining hed not played by societys accepted rules of sexual conduct. That criticism continued while they ignored Hillarys longtime enabling of her husbands abusive sexual behavior and her documented harassment of his victims as well as her own sexual escapades. The media wanted to turn this sexual matter into the central rule of law issue while a far more important rule of law issue regarding the nations foundation still sits on the table. As Roger L. Simon of pjmedia.com wrote on March 23, 2016, The very backbone of our country was the rule of law. Without [it] America as we know it does not exist. Simon declared Americans must be convinced that the FBI investigation of Hillary Clintons email matter was handled justly and that Obama dare not interfere. Neither condition was satisfied. Obama avowed long before the Department of Justice ruled that Hillary was innocent. Then FBI Director James Comey in July 2016 detailed many of Hillarys rule of law violations and in a denial confessionreading through his protests too muchrevealed she should have been prosecuted although Comey lacked the courage to see it through. (On October 28 th under pressure from newly discovered emails he reopened the investigation.) As Simon warned, Democrats nominated an illegal candidate who should have been tried for breaking the law. Trump reiterated that Hillary shouldnt have been allowed to run. Trump also reading Obamas law-breaking Yet theres a deeper, far more powerful story Trump unconsciously hints at one which terrifies Democrats and the entire political class. Trump demonstrates his subtle super-intel instincts. Hes far ahead of the medias conscious take. Deep down, Trump focused on Barack Obama who facilitated Hillary Clintons violation of the law in her email scandal. Trump alludes to a far more significant rule of law issue. His super-intel continues asking the greatest question regarding the legality of any presidential election in U.S. history: the question of Obamas citizenship in 2008 and 2012. From 2008 to 2015, Donald Trump stood strong, demonstrating presidential-type leadership by insisting on respect for the Constitution. He questioned Obamas failure to produce evidence of legal U.S. citizenship. When Obama produced his alleged birth certificate under pressure in 2011, Trump was aware that Sheriff Joe Arpaios panel of document examiners declared it a phony . The media buried the issue. Reporters abused millions of Americans, including Trump, by labeling them racist conspiracy theoristsa basic projection reflecting that the PC media was itself enabling the Obama conspiracy at the expense of our Constitution. Dont believe Trumps no longer a birther. Only recently, to avoid media harassment, did he claim that he accepted Obamas U.S. citizenship. But besides the birth issue, Trump saw that Obama continually functioned as an illegal president . To Americas detriment he repeatedly violated laws on a whim, for example by ignoring immigration laws. When Obama continually neglected to protect Americas national security, Trump confronted him. Trump recognized Obama had increasingly enabled Islamic terrorists while simultaneously building up terrorist Iran, even giving them a nuclear bomb on a layaway plan and cash money to boot. Following the June 11, 2016 Orlando massacre perpetrated by an Islamic terrorist, Trump confronted Obama, observing that Obama wouldnt identify the enemycall them by their name, Islamic terroristsand Trump suggested Obama unconsciously knew the many reasons he should resign the presidency. On the surface, Obama became enraged, but unconsciously Trump elicited from Obama the most shocking confession imaginable. Secretly Obama wants the truth known. On the heels of Trumps challenge, Obama explained between the lines of two speeches on June 12 and 14, that he was truly a mind-controlled Stockholm Syndrome prisoner of radical Islam, programmed at an early age in his own home. The decoded details are described in my book, The Stockholm Syndrome President, How Trump Triggered Obamas Hidden Confession . As he intuitively put together the story of his life, Obama learned the horrifying family secret that his Muslim father had wanted him aborted. (My scientific forensic profiling approach confirmed that Obamas super-intel imagery pointed toward a near abortion which matched his lifes circumstances including a promiscuous 17 year-old mother unexpectedly pregnant from a casual affair.) His father had also emotionally killed him by abandoning him. Obama felt that pain on a daily basis, and then he learned that his father physically abused his mother and two later wives in Kenya. The near-death by abortion and constant death experience at the hands of his anti-American Muslim father became fixated in Obamas mind. He became frozen in terrorsuffering a deep early-childhood traumaterrified that his father would kill him at any moment. When he was a child, his father was radical Islam to himboth one and the same. To stay alive Obama became an unconscious Stockholm Syndrome victim controlled by Islamexplaining his repeated capitulations to them as president and his curious refusal to use that term. Obamas Stockholm Syndrome remains buried deep in his psyche. Only his unconscious super-intel can tell his story in its psycholinguistic language. In his recent speeches, he unknowingly confessed in key images such as doing the terrorists work for them or key denials then the terrorists would win. Im not going to let that happen. But Obama further confessed in his June 14 speech, that he was programmed as a radical Islam mole to carry out a major robbery from America first by stealing the rule of law and running as an illegal president. In fact, Obama has robbed America in many ways: economically with massive debt and hampering business; robbed us of our borders with massive illegal immigration; of gains in racial harmony; of safe communities, of police safety, of inner city safety, of affordable health care, of military power and of our world leadership role. All the while his policies have strengthened terrorist Iran and radical Islam. Now return to Trumps threatened refusal to accept the election results. Consider his brilliant super-intel message to the media: You keep America in suspense about how you avoid the rule of law both with Hillary Clinton and especially Obama, and I will keep you in suspense as to what Ill do about it. Consciously Trump may not yet fully appreciate his deeper plans, but he hints at them strongly. If Trump wins he could appoint a special committee of document examiners to study Obamas birth certificate. As Obama and Clinton have reversed themselves on gay marriage, Trump could simply change his mind about the need to clarify Obamas birthplace. But if Trump loses he could still rise to the occasion by putting rule of law money behind a renewed investigation. He could potentially mobilize an abused citizenry. Rule of law Americans could demand Congress take action and legally obtain Obamas birth recordsas Obama himself, the traumatized child, has unconsciously encouraged. Obamas super-intel, like Trumps and all others, fully espouses the rule of law. And the need for America to re-embrace its moral compass is the most significant story of the election of 2016. Don't forget to Like Freedom Outpost on Facebook , Google Plus , & Twitter . You can also get Freedom Outpost delivered to your Amazon Kindle device here . shares"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = data.text[151]\n",
    "\n",
    "def pre_process(data_text):\n",
    "    ex = preprocess_text(data_text, remove_special = False, stem=False, lemmatize = False, remove_stops = False)\n",
    "    ex = ex.replace(\"\\n\", \"\")\n",
    "    ex = ex.replace(\"\\r\", \"\")\n",
    "    doc = nlp(ex)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "cleaned_text = pre_process(data_text)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text'] = data['text'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Post-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_and_tag(text):\n",
    "    tag_arr = np.zeros(0)\n",
    "    pos_arr = np.zeros(0)\n",
    "    for i in text:\n",
    "        pos_arr = np.append(pos_arr, i.pos_)\n",
    "        tag_arr = np.append(tag_arr, i.tag_)        \n",
    "    return (pos_arr, tag_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(raw_text): #raw_text is original text   \n",
    "    text = pre_process(raw_text)\n",
    "    words = [token.text for token in text if token.is_punct != True]\n",
    "    word_counter = Counter(([word for word in words]))\n",
    "    sorted_word_counts = list(sorted(word_counter.values(), reverse = True)) #sorted in descending order\n",
    "    \n",
    "    pos_arr, tag_arr = get_pos_and_tag(text)\n",
    "    pos_counter = Counter(([pos for pos in pos_arr]))\n",
    "    tag_counter = Counter(([tag for tag in tag_arr]))\n",
    "\n",
    "    total_count = sum(tag_counter.values()) #same for tag and pos\n",
    "    \n",
    "    ###features\n",
    "    total_word_count = sum(word_counter.values())\n",
    "    \n",
    "    if total_word_count == 0:\n",
    "        avg_word_length = 0\n",
    "        lexical_diversity = 0\n",
    "        repetition_top = 0\n",
    "        repetition_all = 0\n",
    "    else:\n",
    "        avg_word_length = sum(len(word) for word in words)/total_word_count\n",
    "        lexical_diversity = len(word_counter)/total_word_count\n",
    "        #sum of number of words of top 20 words seen over total number of words\n",
    "        repetition_top = sum(sorted_word_counts[:20])/total_word_count\n",
    "        #1/k weighting on sum of word counts over total number of words\n",
    "        repetition_all = sum(sorted_word_counts[i]/(i+1) for i in range(len(sorted_word_counts)))/total_word_count\n",
    "    \n",
    "    if total_count == 0:\n",
    "        NNP_percent = 0\n",
    "        NNPS_percent = 0\n",
    "        noun_percent = 0\n",
    "        verb_percent = 0\n",
    "        part_percent = 0\n",
    "        det_percent = 0\n",
    "        unknown_or_foreign_percent = 0\n",
    "    else:\n",
    "        #tag percents\n",
    "        NNP_percent = tag_counter.get(\"NNP\", 0)/total_count\n",
    "        NNPS_percent = tag_counter.get(\"NNPS\", 0)/total_count\n",
    "        #POS percents\n",
    "        noun_percent = pos_counter.get(\"NOUN\", 0)/total_count\n",
    "        verb_percent = pos_counter.get(\"VERB\", 0)/total_count\n",
    "        part_percent = pos_counter.get(\"PART\", 0)/total_count\n",
    "        det_percent = pos_counter.get(\"DET\", 0)/total_count\n",
    "        unknown_or_foreign_percent = pos_counter.get(\"X\", 0)/total_count\n",
    "        \n",
    "    return [total_word_count,avg_word_length,lexical_diversity,repetition_top,repetition_all,\\\n",
    "            NNP_percent,NNPS_percent,noun_percent,verb_percent,part_percent,det_percent,unknown_or_foreign_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {}\n",
    "\n",
    "dct['total_word_count'],dct['avg_word_length'],dct['lexical_diversity'],dct['repetition_top'],\\\n",
    "dct['repetition_all'],dct['NNP_percent'],dct['NNPS_percent'],dct['noun_percent'],\\\n",
    "dct['verb_percent'],dct['part_percent'],dct['det_percent'],dct['unknown_or_foreign_percent']\\\n",
    "= zip(*data.iloc[10000:].text.apply(get_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5679, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>repetition_top</th>\n",
       "      <th>repetition_all</th>\n",
       "      <th>NNP_percent</th>\n",
       "      <th>NNPS_percent</th>\n",
       "      <th>noun_percent</th>\n",
       "      <th>verb_percent</th>\n",
       "      <th>part_percent</th>\n",
       "      <th>det_percent</th>\n",
       "      <th>unknown_or_foreign_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.292897</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2465</td>\n",
       "      <td>4.956592</td>\n",
       "      <td>0.382556</td>\n",
       "      <td>0.345233</td>\n",
       "      <td>0.139596</td>\n",
       "      <td>0.099549</td>\n",
       "      <td>0.006386</td>\n",
       "      <td>0.182569</td>\n",
       "      <td>0.126972</td>\n",
       "      <td>0.019534</td>\n",
       "      <td>0.131104</td>\n",
       "      <td>0.000751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>345</td>\n",
       "      <td>4.162319</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.119176</td>\n",
       "      <td>0.049351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166234</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.031169</td>\n",
       "      <td>0.098701</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>932</td>\n",
       "      <td>5.101931</td>\n",
       "      <td>0.443133</td>\n",
       "      <td>0.335837</td>\n",
       "      <td>0.136687</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.201951</td>\n",
       "      <td>0.149268</td>\n",
       "      <td>0.018537</td>\n",
       "      <td>0.126829</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>569</td>\n",
       "      <td>4.889279</td>\n",
       "      <td>0.562390</td>\n",
       "      <td>0.328647</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>0.135937</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.151562</td>\n",
       "      <td>0.151562</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_word_count  avg_word_length  lexical_diversity  repetition_top  \\\n",
       "0                10         5.200000           1.000000        1.000000   \n",
       "1              2465         4.956592           0.382556        0.345233   \n",
       "2               345         4.162319           0.565217        0.382609   \n",
       "3               932         5.101931           0.443133        0.335837   \n",
       "4               569         4.889279           0.562390        0.328647   \n",
       "\n",
       "   repetition_all  NNP_percent  NNPS_percent  noun_percent  verb_percent  \\\n",
       "0        0.292897     0.545455      0.000000      0.090909      0.000000   \n",
       "1        0.139596     0.099549      0.006386      0.182569      0.126972   \n",
       "2        0.119176     0.049351      0.000000      0.166234      0.207792   \n",
       "3        0.136687     0.120000      0.002927      0.201951      0.149268   \n",
       "4        0.110294     0.135937      0.009375      0.151562      0.151562   \n",
       "\n",
       "   part_percent  det_percent  unknown_or_foreign_percent  \n",
       "0      0.000000     0.090909                    0.000000  \n",
       "1      0.019534     0.131104                    0.000751  \n",
       "2      0.031169     0.098701                    0.000000  \n",
       "3      0.018537     0.126829                    0.000000  \n",
       "4      0.025000     0.118750                    0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"postag_output_10000_end.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  can ignore all below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noun percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "1606\n",
      "0.0896637608966376\n"
     ]
    }
   ],
   "source": [
    "def NNP_percent(postag_count):\n",
    "    return postag_count.get(\"NNP\") / sum(postag_count.values())\n",
    "\n",
    "def NNPS_percent(postag_count):\n",
    "    return postag_count.get(\"NNPS\") / sum(postag_count.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General version (postag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postag_percent(postag_count, postag_string):\n",
    "    return postag_count.get(postag_string) / sum(postag_count.values())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verb count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postag(doc):\n",
    "    pos_arr = np.zeros(0)\n",
    "    for i in doc:\n",
    "        pos_arr = np.append(postag_arr, i.pos_)\n",
    "    \n",
    "    return pos_arr\n",
    "    \n",
    "\n",
    "def verb_count(pos_arr):\n",
    "    pos_count = Counter(([pos for pos in pos_arr]))\n",
    "    verb_count = pos_count.get(\"VERB\") / sum(pos_count.values())\n",
    "    # print(pos_count)\n",
    "    # print(pos_count.get(\"VERB\"))\n",
    "    # print(sum(pos_count.values()))\n",
    "    # print(verb_count)\n",
    "    \n",
    "    return verb_count\n",
    "\n",
    "#(ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NOUN', 'NOUN', 'PROPN', ..., 'ADJ', 'NOUN', 'PUNCT'], dtype='<U32')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General version (pos) (less powerful version of postag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_count(pos_arr, pos_string):\n",
    "    pos_count = Counter(([pos for pos in pos_arr]))\n",
    "    result = pos_count.get(pos_string) / sum(pos_count.values())\n",
    "    # print(pos_count)\n",
    "    # print(pos_count.get(\"VERB\"))\n",
    "    # print(sum(pos_count.values()))\n",
    "    # print(verb_count)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.136521136521137\n"
     ]
    }
   ],
   "source": [
    "# token.is_stop != True\n",
    "words =  [token.text for token in doc if token.is_punct != True]\n",
    "# print(words)\n",
    "\n",
    "def average_word_length(words):\n",
    "    count = 0\n",
    "    \n",
    "    for word in words:     \n",
    "        count += len(word)\n",
    "        \n",
    "    return count / len(words)\n",
    "\n",
    "# print(words)\n",
    "print(average_word_length(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.137777777777778\n"
     ]
    }
   ],
   "source": [
    "def average_word_count(word_counter):\n",
    "    # print(word_counter.values())\n",
    "    return np.array(list(word_counter.values())).mean()\n",
    "\n",
    "# print(words)\n",
    "print(average_word_count(word_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443\n"
     ]
    }
   ],
   "source": [
    "def word_count(word_counter):\n",
    "    return sum(word_counter.values())\n",
    "\n",
    "# print(word_count(words))\n",
    "print(sum(word_counter.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686\n"
     ]
    }
   ],
   "source": [
    "def unique_words(word_counter):\n",
    "    return len(word_counter)\n",
    "\n",
    "print(unique_words(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(word_counter):\n",
    "    return unique_words(word_counter) / word_count(word_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)\n",
    "\n",
    "def sentence_count(doc):\n",
    "    return len(list(doc.sents))\n",
    "\n",
    "sentence_count(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6ab5fcfe59bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data.text, data.label\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=0, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27208,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30232,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag_arr = np.zeros(0)\n",
    "for i in doc:\n",
    "    postag_arr = np.append(postag_arr, i.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NN', 'NN', 'NNP', ..., 'JJ', 'NN', '.'], dtype='<U32')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postag_arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
