{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from lib.processor import preprocess_text\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  In an interview with congressional investigato...      0\n",
      "1  Getty - John Shearer / Staff \\r\\nComedian Patt...      1\n",
      "2  By Jameson Parker Election 2016 , Politics Nov...      1\n",
      "3   Last updated at 16:53 GMT A helmet for cyclis...      0\n",
      "4  13 Herbal Teas With Highest Antioxidants http:...      1\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(BASE_DIR, 'dataset.csv'), sep=',')\n",
    "print(data.head())\n",
    "data.text = data.text.map(lambda x: preprocess_text(x, True, True, False, True, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  interview congressional investigator diplomat ...      0\n",
      "1  Getty John Shearer Staff Comedian Patton Oswal...      1\n",
      "2  Jameson Parker Election 2016 Politics November...      1\n",
      "3  update 1653 GMT helmet cyclist spot imminent c...      0\n",
      "4  13 Herbal Teas Highest Antioxidants httpblogsn...      1\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(BASE_DIR,'data','wj_processed.csv'), sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, text) for text in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and tokenize the dataset for w2v\n",
    "#train, test = train_test_split(data, test_size=0.3, random_state = 42)\n",
    "\n",
    "#test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "#train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "data_tokenized = data.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained model\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(BASE_DIR,\"models\",\"GoogleNews-vectors-negative300.bin.gz\"), binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word_average = word_averaging_list(wv, data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0288958   0.04923387  0.03451601 ... -0.05211262  0.04874931\n",
      "  -0.01187791]\n",
      " [ 0.03234296  0.0256858  -0.03558049 ... -0.05346137  0.0661214\n",
      "  -0.01541387]\n",
      " [ 0.07398563  0.01908754 -0.00030894 ... -0.06885757  0.00210524\n",
      "   0.00119012]\n",
      " [-0.00864953 -0.02571224 -0.00632877 ...  0.00580416  0.05334501\n",
      "  -0.09583674]\n",
      " [-0.04196492  0.04734785 -0.02476608 ...  0.06400798  0.06183368\n",
      "   0.06039234]]\n"
     ]
    }
   ],
   "source": [
    "print(data_word_average[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15679, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word_average.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0  0.028896  0.049234  0.034516  0.055213 -0.105407 -0.046933  0.025233   \n",
      "1  0.032343  0.025686 -0.035580  0.104289 -0.042332 -0.003351  0.064108   \n",
      "2  0.073986  0.019088 -0.000309  0.055446 -0.021151  0.014929  0.045064   \n",
      "3 -0.008650 -0.025712 -0.006329 -0.010441 -0.097033 -0.047436  0.067023   \n",
      "4 -0.041965  0.047348 -0.024766  0.090478 -0.041287  0.065566  0.045625   \n",
      "\n",
      "        7         8         9      ...          290       291       292  \\\n",
      "0 -0.109444  0.141332  0.047805    ...    -0.037357 -0.022533 -0.076386   \n",
      "1 -0.085843  0.125483  0.033254    ...    -0.032302  0.009351 -0.131091   \n",
      "2 -0.007039  0.130749  0.065265    ...    -0.000070 -0.008963 -0.106510   \n",
      "3 -0.086308  0.152578  0.020291    ...     0.006758 -0.013189 -0.129570   \n",
      "4 -0.098067  0.058216  0.062641    ...     0.001415 -0.025904 -0.015243   \n",
      "\n",
      "        293       294       295       296       297       298       299  \n",
      "0  0.008189 -0.036483 -0.023470 -0.046182 -0.052113  0.048749 -0.011878  \n",
      "1 -0.007809 -0.055201 -0.054668 -0.039017 -0.053461  0.066121 -0.015414  \n",
      "2 -0.019064 -0.009824 -0.070133  0.026817 -0.068858  0.002105  0.001190  \n",
      "3  0.001201 -0.007650 -0.061370 -0.029110  0.005804  0.053345 -0.095837  \n",
      "4 -0.025034 -0.053519  0.029189 -0.050209  0.064008  0.061834  0.060392  \n",
      "\n",
      "[5 rows x 300 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(BASE_DIR,'data','w2v_300_dim.csv'), sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "#X_test_word_average = word_averaging_list(wv,test_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8548044217687075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.87      0.90      0.89      3005\n",
      "        fake       0.82      0.77      0.79      1699\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      4704\n",
      "   macro avg       0.85      0.84      0.84      4704\n",
      "weighted avg       0.85      0.85      0.85      4704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=2,solver='lbfgs', C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['label'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.label))\n",
    "print(classification_report(test.label, y_pred,target_names=['real','fake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
