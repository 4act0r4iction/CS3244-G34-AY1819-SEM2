{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Must haves:\n",
    "1. **\"combined_data_no_duplis.csv\"**. I sent that on whatsapp chat\n",
    "2. **\"processor.py\"**. Import this module. Is uploaded by Wei Jie on Git. I used it to expand contractions and remove accented characters.\n",
    "3. **\"stanfordcorenlp\"**. Please do pip install stanfordcorenlp\n",
    "4. The folder **stanford-corenlp-full-2018-10-05**. Download **stanford-corenlp-full-2018-10-05.zip** and extract the whole folder to somewhere on your computer. Go to https://stanfordnlp.github.io/CoreNLP/download.html and click \"Download CoreNLP 3.9.2\"\n",
    "5. Other packages that y'all definitely have already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from lib.processor import * # Import \"processor.py\" based on its location in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\") # Import dataset based on its location in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15679, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset to run!\n",
    "1. Jingyang: **[:3500]**\n",
    "2. Guanhao: **[3500:7000]**\n",
    "3. Joel: **[7000:10500]**\n",
    "4. Weijie: **[10500:]**\n",
    "\n",
    "**\"Un-comment\" the relevant line in the next cell!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.iloc[:3500,]\n",
    "# data = data.iloc[3500:7000,]\n",
    "# data = data.iloc[7000:10500,]\n",
    "# data = data.iloc[10500:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_sentences'] = data.text.apply(split_into_sentences)\n",
    "data['total_sentences'] = data['total_sentences'].apply(len)\n",
    "data['num_sentences_with_adj_in_phrase'] = 0\n",
    "data['num_sentences_with_adv_in_phrase'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change path below according to your desktop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stanford-corenlp-full-2018-10-05']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../../../Downloads/stanford-corenlp-full-2018-10-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk import Tree\n",
    "\n",
    "nlp2 = StanfordCoreNLP('../../../Downloads/stanford-corenlp-full-2018-10-05/stanford-corenlp-full-2018-10-05')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Traditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function below to check presence of **Adjectives** and **Adverbs** respectively, in **NP/VP**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj(x):\n",
    "    x = expand_contractions(remove_accented_chars(x))\n",
    "    if (\"JJ\" in x):\n",
    "        if ((\"VP\" in x) | (\"NP\" in x)):\n",
    "            if ((\"ADVP\" not in x) & (\"WHAVP\" not in x) & (\"WHNP\" not in x)):\n",
    "                return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv(x):\n",
    "    x = expand_contractions(remove_accented_chars(x))\n",
    "    if (\"RB\" in x):\n",
    "        if ((\"VP\" in x) | (\"NP\" in x)):\n",
    "            if ((\"ADVP\" not in x) & (\"WHAVP\" not in x) & (\"WHNP\" not in x)):\n",
    "                return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop to add the results into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for j in range(len(data.text)):\n",
    "    adjcounter = 0\n",
    "    advcounter = 0\n",
    "    try:\n",
    "        for k in (split_into_sentences(data.iloc[j,0])):\n",
    "            temptree = Tree.fromstring(nlp2.parse(k))\n",
    "            productions = temptree.productions()\n",
    "            adjstatus = False\n",
    "            advstatus = False\n",
    "            for i in range(len(productions)):\n",
    "                if (adj(str(productions[i]))):\n",
    "                    adjstatus = True\n",
    "                if (adv(str(productions[i]))):\n",
    "                    advstatus = True\n",
    "            if (adjstatus):\n",
    "                adjcounter += 1\n",
    "            if (advstatus):\n",
    "                advcounter += 1\n",
    "                    \n",
    "        data.iloc[j, 3] = adjcounter\n",
    "        data.iloc[j, 4] = advcounter\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your table as e.g. \"data_jingyang.csv\"\n",
    "\n",
    "**\"Un-comment\" the relevant line in the next cell!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2.close()   # Website say not doing that will waste backend memory\n",
    "# data.to_csv(\"data_jingyang.csv\")\n",
    "# data.to_csv(\"data_guanhao.csv\")\n",
    "# data.to_csv(\"data_joel.csv\")\n",
    "# data.to_csv(\"data_weijie.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Ignore below!!</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: No loops but slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(x):\n",
    "    if (adj(str(x))):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "vectorize_output = (np.vectorize)(func2)\n",
    "\n",
    "def funcon1(x):\n",
    "    productions = np.asarray(Tree.fromstring(nlp2.parse(x)).productions())\n",
    "    if (sum(vectorize_output(productions)>=1)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "vectorized_funcon1 = np.vectorize(funcon1)\n",
    "np.sum(vectorized_funcon1(sentences_array[0]))\n",
    "\n",
    "sentences_array = data.text.apply(split_into_sentences)\n",
    "\n",
    "vectorized_funcon1 = np.vectorize(funcon1)\n",
    "vectorized_eval = np.vectorize(lambda x: np.sum(vectorized_funcon1(x)))\n",
    "start = time.time()\n",
    "print(vectorized_eval(sentences_array[0:10]))\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
