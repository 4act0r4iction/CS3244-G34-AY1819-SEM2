{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import gensim\n",
    "import time\n",
    "#sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import log_loss, recall_score, precision_score, classification_report\n",
    "from sklearn.externals import joblib\n",
    "from lib.processor import *\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp2 = StanfordCoreNLP('../../../Downloads/stanford-corenlp-full-2018-10-05/stanford-corenlp-full-2018-10-05')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\", sep = \",\")\n",
    "df_small = df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final feature extractor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smart_open.smart_open_lib:this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(\"models\",\"GoogleNews-vectors-negative300.bin.gz\"), binary=True)\n",
    "wv.init_sims(replace=True)\n",
    "\n",
    "tfidf_logreg_model = joblib.load(\"models/tfidf/tfidf_logreg_model.pkl\")\n",
    "tfidf_nb_model = joblib.load(\"models/tfidf/tfidf_nb_model.pkl\")\n",
    "w2v_logreg_model = joblib.load(\"models/w2v/w2v_logreg_model.pkl\")\n",
    "w2v_nb_model = joblib.load(\"models/w2v/w2v_nb_model.pkl\")\n",
    "w2v_rforest_model = joblib.load(\"models/w2v/w2v_rforest_model.pkl\")\n",
    "w2v_svc_model = joblib.load(\"models/w2v/w2v_svc_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCFG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return np.array(sentences)\n",
    "\n",
    "def adj(x):\n",
    "    x = expand_contractions(remove_accented_chars(x))\n",
    "    if (\"JJ\" in x):\n",
    "        if ((\"VP\" in x) | (\"NP\" in x)):\n",
    "            if ((\"ADVP\" not in x) & (\"WHAVP\" not in x) & (\"WHNP\" not in x)):\n",
    "                return True\n",
    "            \n",
    "def adv(x):\n",
    "    x = expand_contractions(remove_accented_chars(x))\n",
    "    if (\"RB\" in x):\n",
    "        if ((\"VP\" in x) | (\"NP\" in x)):\n",
    "            if ((\"ADVP\" not in x) & (\"WHAVP\" not in x) & (\"WHNP\" not in x)):\n",
    "                return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POSTag functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def pre_process(raw_text): #pre-processes the raw text from the dataset\n",
    "    ex = preprocess_text(raw_text, remove_special = False, stem=False, lemmatize = False, remove_stops = False)\n",
    "    ex = ex.replace(\"\\n\", \"\")\n",
    "    ex = ex.replace(\"\\r\", \"\")\n",
    "    doc = nlp(ex)\n",
    "    return doc\n",
    "\n",
    "def get_pos_and_tag(text):\n",
    "    tag_arr = np.zeros(0)\n",
    "    pos_arr = np.zeros(0)\n",
    "    for i in text:\n",
    "        pos_arr = np.append(pos_arr, i.pos_)\n",
    "        tag_arr = np.append(tag_arr, i.tag_)        \n",
    "    return (pos_arr, tag_arr)\n",
    "\n",
    "def get_features(raw_text): #raw_text is original text   \n",
    "    text = pre_process(raw_text)\n",
    "    words = [token.text for token in text if token.is_punct != True]\n",
    "    word_counter = Counter(([word for word in words]))\n",
    "    sorted_word_counts = list(sorted(word_counter.values(), reverse = True)) #sorted in descending order\n",
    "    \n",
    "    pos_arr, tag_arr = get_pos_and_tag(text)\n",
    "    pos_counter = Counter(([pos for pos in pos_arr]))\n",
    "    tag_counter = Counter(([tag for tag in tag_arr]))\n",
    "\n",
    "    total_count = sum(tag_counter.values()) #same for tag and pos\n",
    "    \n",
    "    ###features\n",
    "    total_word_count = sum(word_counter.values())\n",
    "    \n",
    "    if total_word_count == 0:\n",
    "        avg_word_length = 0\n",
    "        lexical_diversity = 0\n",
    "        repetition_top = 0\n",
    "        repetition_all = 0\n",
    "    else:\n",
    "        avg_word_length = sum(len(word) for word in words)/total_word_count\n",
    "        lexical_diversity = len(word_counter)/total_word_count\n",
    "        #sum of number of words of top 20 words seen over total number of words\n",
    "        repetition_top = sum(sorted_word_counts[:20])/total_word_count\n",
    "        #1/k weighting on sum of word counts over total number of words\n",
    "        repetition_all = sum(sorted_word_counts[i]/(i+1) for i in range(len(sorted_word_counts)))/total_word_count\n",
    "    \n",
    "    if total_count == 0:\n",
    "        NNP_percent = 0\n",
    "        NNPS_percent = 0\n",
    "        noun_percent = 0\n",
    "        verb_percent = 0\n",
    "        part_percent = 0\n",
    "        det_percent = 0\n",
    "        unknown_or_foreign_percent = 0\n",
    "    else:\n",
    "        #tag percents\n",
    "        NNP_percent = tag_counter.get(\"NNP\", 0)/total_count\n",
    "        NNPS_percent = tag_counter.get(\"NNPS\", 0)/total_count\n",
    "        #POS percents\n",
    "        noun_percent = pos_counter.get(\"NOUN\", 0)/total_count\n",
    "        verb_percent = pos_counter.get(\"VERB\", 0)/total_count\n",
    "        part_percent = pos_counter.get(\"PART\", 0)/total_count\n",
    "        det_percent = pos_counter.get(\"DET\", 0)/total_count\n",
    "        unknown_or_foreign_percent = pos_counter.get(\"X\", 0)/total_count\n",
    "        \n",
    "    return [total_word_count,avg_word_length,lexical_diversity,repetition_top,repetition_all,\\\n",
    "            NNP_percent,NNPS_percent,noun_percent,verb_percent,part_percent,det_percent,unknown_or_foreign_percent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, text) for text in text_list ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(df): #input is original raw df (with columns labeled as text and label)\n",
    "                            #output is the df of features and label (no text included in output)\n",
    "    x = df.text\n",
    "    y = df.label\n",
    "    \n",
    "    dct = {} #initialize an empty dictionary\n",
    "\n",
    "    #PCFG\n",
    "    df_pcfg = df.drop(['text','label'], axis = 1).copy()\n",
    "    df_pcfg['total_sentences'] = x.apply(split_into_sentences)\n",
    "    df_pcfg['total_sentences'] = df_pcfg['total_sentences'].apply(len)\n",
    "    df_pcfg['num_sentences_with_adj_in_phrase'] = 0\n",
    "    df_pcfg['num_sentences_with_adv_in_phrase'] = 0\n",
    "    \n",
    "    for j in range(len(x)):\n",
    "        adjcounter = 0\n",
    "        advcounter = 0\n",
    "        try:\n",
    "            for k in (split_into_sentences(df.iloc[j,0])):\n",
    "                temptree = Tree.fromstring(nlp2.parse(k))\n",
    "                productions = temptree.productions()\n",
    "                adjstatus = False\n",
    "                advstatus = False\n",
    "                for i in range(len(productions)):\n",
    "                    if (adj(str(productions[i]))):\n",
    "                        adjstatus = True\n",
    "                    if (adv(str(productions[i]))):\n",
    "                        advstatus = True\n",
    "                if (adjstatus):\n",
    "                    adjcounter += 1\n",
    "                if (advstatus):\n",
    "                    advcounter += 1\n",
    "\n",
    "            df_pcfg.iloc[j, 1] = adjcounter\n",
    "            df_pcfg.iloc[j, 2] = advcounter\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    dct['total_sentences'] = df_pcfg['total_sentences']\n",
    "    dct['num_sentences_with_adj_in_phrase'] = df_pcfg['num_sentences_with_adj_in_phrase']\n",
    "    dct['num_sentences_with_adv_in_phrase'] = df_pcfg['num_sentences_with_adv_in_phrase']\n",
    "    \n",
    "    #POStag\n",
    "    dct['total_word_count'],dct['avg_word_length'],dct['lexical_diversity'],dct['repetition_top'],\\\n",
    "    dct['repetition_all'],dct['NNP_percent'],dct['NNPS_percent'],dct['noun_percent'],\\\n",
    "    dct['verb_percent'],dct['part_percent'],dct['det_percent'],dct['unknown_or_foreign_percent']\\\n",
    "    = zip(*x.apply(get_features))\n",
    "    \n",
    "    #tfidf\n",
    "    dct['tfidf_prob_nb'] = tfidf_nb_model.predict_proba(x)[:,1]\n",
    "    dct['tfidf_prob_logreg'] = tfidf_logreg_model.predict_proba(x)[:,1]\n",
    "    \n",
    "    #w2v\n",
    "    w2v_text = df.text.map(lambda x: preprocess_text(x, True, True, False, True, True))\n",
    "    w2v_tokenized = w2v_text.map(w2v_tokenize_text).values\n",
    "    data_word_average = word_averaging_list(wv, w2v_tokenized)\n",
    "    x_w2v = pd.DataFrame(data_word_average)\n",
    "    dct['w2v_prob_nb'] = w2v_nb_model.predict_proba(x_w2v)[:,1]\n",
    "    dct['w2v_prob_logreg'] = w2v_logreg_model.predict_proba(x_w2v)[:,1]\n",
    "    dct['w2v_prob_svc'] = w2v_svc_model.predict_proba(x_w2v)[:,1]\n",
    "    dct['w2v_prob_rforest'] = w2v_rforest_model.predict_proba(x_w2v)[:,1]\n",
    "    \n",
    "    dct['is_fake'] = y\n",
    "   \n",
    "    return pd.DataFrame(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>num_sentences_with_adj_in_phrase</th>\n",
       "      <th>num_sentences_with_adv_in_phrase</th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>repetition_top</th>\n",
       "      <th>repetition_all</th>\n",
       "      <th>NNP_percent</th>\n",
       "      <th>NNPS_percent</th>\n",
       "      <th>...</th>\n",
       "      <th>part_percent</th>\n",
       "      <th>det_percent</th>\n",
       "      <th>unknown_or_foreign_percent</th>\n",
       "      <th>tfidf_prob_nb</th>\n",
       "      <th>tfidf_prob_logreg</th>\n",
       "      <th>w2v_prob_nb</th>\n",
       "      <th>w2v_prob_logreg</th>\n",
       "      <th>w2v_prob_svc</th>\n",
       "      <th>w2v_prob_rforest</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>1549</td>\n",
       "      <td>4.571336</td>\n",
       "      <td>0.371853</td>\n",
       "      <td>0.339574</td>\n",
       "      <td>0.130808</td>\n",
       "      <td>0.111924</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028121</td>\n",
       "      <td>0.113611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.171641e-03</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>5.938140e-04</td>\n",
       "      <td>0.440473</td>\n",
       "      <td>0.349879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>488</td>\n",
       "      <td>4.534836</td>\n",
       "      <td>0.561475</td>\n",
       "      <td>0.315574</td>\n",
       "      <td>0.108682</td>\n",
       "      <td>0.099822</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.980483e-01</td>\n",
       "      <td>0.637621</td>\n",
       "      <td>9.999960e-01</td>\n",
       "      <td>0.825876</td>\n",
       "      <td>0.883786</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>443</td>\n",
       "      <td>4.458239</td>\n",
       "      <td>0.577878</td>\n",
       "      <td>0.347630</td>\n",
       "      <td>0.107576</td>\n",
       "      <td>0.099174</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.128099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.972521e-01</td>\n",
       "      <td>0.865423</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.977274</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>4.344262</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.136849</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.997447e-10</td>\n",
       "      <td>0.042518</td>\n",
       "      <td>1.646796e-09</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>505</td>\n",
       "      <td>4.879208</td>\n",
       "      <td>0.534653</td>\n",
       "      <td>0.328713</td>\n",
       "      <td>0.109228</td>\n",
       "      <td>0.053819</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.064236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.937735</td>\n",
       "      <td>9.999966e-01</td>\n",
       "      <td>0.823087</td>\n",
       "      <td>0.874765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_sentences  num_sentences_with_adj_in_phrase  \\\n",
       "0               76                                33   \n",
       "1               27                                12   \n",
       "2               20                                13   \n",
       "3                3                                 1   \n",
       "4               50                                20   \n",
       "\n",
       "   num_sentences_with_adv_in_phrase  total_word_count  avg_word_length  \\\n",
       "0                                19              1549         4.571336   \n",
       "1                                 8               488         4.534836   \n",
       "2                                 3               443         4.458239   \n",
       "3                                 0                61         4.344262   \n",
       "4                                 3               505         4.879208   \n",
       "\n",
       "   lexical_diversity  repetition_top  repetition_all  NNP_percent  \\\n",
       "0           0.371853        0.339574        0.130808     0.111924   \n",
       "1           0.561475        0.315574        0.108682     0.099822   \n",
       "2           0.577878        0.347630        0.107576     0.099174   \n",
       "3           0.836066        0.491803        0.136849     0.169231   \n",
       "4           0.534653        0.328713        0.109228     0.053819   \n",
       "\n",
       "   NNPS_percent  ...  part_percent  det_percent  unknown_or_foreign_percent  \\\n",
       "0      0.007312  ...      0.028121     0.113611                         0.0   \n",
       "1      0.003565  ...      0.030303     0.117647                         0.0   \n",
       "2      0.004132  ...      0.008264     0.128099                         0.0   \n",
       "3      0.015385  ...      0.030769     0.138462                         0.0   \n",
       "4      0.003472  ...      0.008681     0.064236                         0.0   \n",
       "\n",
       "   tfidf_prob_nb  tfidf_prob_logreg   w2v_prob_nb  w2v_prob_logreg  \\\n",
       "0   3.171641e-03           0.206800  5.938140e-04         0.440473   \n",
       "1   9.980483e-01           0.637621  9.999960e-01         0.825876   \n",
       "2   9.972521e-01           0.865423  1.000000e+00         0.977274   \n",
       "3   6.997447e-10           0.042518  1.646796e-09         0.021398   \n",
       "4   1.000000e+00           0.937735  9.999966e-01         0.823087   \n",
       "\n",
       "   w2v_prob_svc  w2v_prob_rforest  is_fake  \n",
       "0      0.349879               0.5        0  \n",
       "1      0.883786               0.6        1  \n",
       "2      0.996848               0.9        1  \n",
       "3      0.001886               0.0        0  \n",
       "4      0.874765               1.0        1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test on small (5 entries) data; takes about 15 seconds (thanks to pcfg lol)\n",
    "feature_extractor(df_small)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
